# Qwen Studio ğŸš€

A polished, modern AI-powered coding assistant built leveraging local LLMs via Ollama. It features a beautiful, responsive UI inspired by top-tier AI platforms, refined with TypeScript and React.

## âœ¨ Features

- **ğŸ›¡ï¸ Modern Tech Stack**: Built with React, TypeScript, Vite, and Tailwind CSS v4.
- **ğŸ¨ Beautiful UI**: Glassmorphism, smooth gradients, and typography.
- **ğŸ“± Fully Responsive**: Optimized for both desktop and mobile devices.
- **ğŸŒ™ Dark/Light Mode**: Seamless theme switching with system preference detection.
- **ğŸ’¾ Persistent History**: Auto-saves conversations locally using IndexedDB.
- **âš¡ Real-time Streaming**: Instant response streaming directly from local Ollama.
- **ğŸ“Š Generation Stats**: See token count, speed (t/s), and duration for every response.
- **ğŸ“± PWA Support**: Installable as a native-like app on your device.
- **ğŸ“ Markdown Support**: Full code syntax highlighting and copy-to-clipboard functionality.
- **ğŸ›ï¸ Model Switching**: Easily switch between installed Ollama models on the fly.

## ğŸš€ Quick Start

### Prerequisites

1.  **Ollama**: Ensure [Ollama](https://ollama.com/) is installed and running.
    ```bash
    ollama serve
    ```
2.  **Node.js**: Version 18+ recommended.

### Installation

1.  **Clone the repository** (if applicable) or navigate to the project folder.

2.  **Install Dependencies**:
    ```bash
    npm install
    ```

### Running the Application

1.  **Start the Development Server**:
    ```bash
    npm run dev
    ```

2.  **Open your browser**: Navigate to `http://localhost:5173`

## ğŸ—ï¸ Architecture

-   **Frontend**: React (TypeScript), Vite, Tailwind CSS, IDB (IndexedDB wrapper).
-   **AI Engine**: Ollama (Local LLM), accessed via Vite development proxy.
-   **Storage**: Browser's IndexedDB for chat history persistence.

## ğŸ› ï¸ Configuration

-   **Proxy**: The Vite dev server proxies `/api` to `http://localhost:11434/api` to communicate with Ollama directly.
-   **Models**: The app automatically fetches available models from your local Ollama instance.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT License.