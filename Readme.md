# Qwen Studio ğŸš€

A polished, modern AI-powered coding assistant built leveraging local LLMs via Ollama. It features a beautiful, responsive UI inspired by top-tier AI platforms, refined with TypeScript and React.

![Qwen Studio](https://via.placeholder.com/1200x600?text=Qwen+Studio+Interface)

## âœ¨ Features

- **ğŸ›¡ï¸ Modern Tech Stack**: Built with React, TypeScript, Vite, and Tailwind CSS v4.
- **ğŸ¨ Beautiful UI**: Glassmorphism, smooth gradients, and Inter typography.
- **ğŸ“± Fully Responsive**: Optimized for both desktop and mobile devices.
- **ğŸŒ™ Dark/Light Mode**: Seamless theme switching with system preference detection.
- **ğŸ’¾ Persistent History**: Auto-saves conversations locally using IndexedDB.
- **âš¡ Real-time Streaming**: Instant response streaming from local Ollama usage.
- **ğŸ“Š Generation Stats**: See token count, speed (t/s), and duration for every response.
- **ğŸ“± PWA Support**: Installable as a native-like app on your device.
- **ğŸ“ Markdown Support**: Full code syntax highlighting and copy-to-clipboard functionality.
- **ğŸ›ï¸ Model Switching**: Easily switch between installed Ollama models on the fly.

## ğŸš€ Quick Start

### Prerequisites

1.  **Ollama**: Ensure [Ollama](https://ollama.com/) is installed and running.
    ```bash
    ollama serve
    ```
2.  **Node.js**: Version 16+ is required.

### Installation

1.  **Clone the repository** (if applicable) or navigate to the project folder.

2.  **Install Client Dependencies**:
    ```bash
    cd client
    npm install
    ```

3.  **Install Server Dependencies**:
    ```bash
    cd ..
    npm install
    ```

### Running the Application

1.  **Start the Backend Server** (Port 14000):
    ```bash
    # In the root directory
    npm start
    ```

2.  **Start the Frontend Client** (Port 5173):
    ```bash
    # In the client directory
    npm run dev
    ```

3.  **Open your browser**: Navigate to `http://localhost:5173`

## ğŸ—ï¸ Architecture

-   **Frontend**: React (TypeScript), Vite, Tailwind CSS, IDB (IndexedDB wrapper).
-   **Backend**: Hono (Node.js), serving as a proxy to Ollama and handling stats.
-   **AI Engine**: Ollama (Local LLM), running on port 11434.
-   **Storage**: Browser's IndexedDB for chat history.

## ğŸ› ï¸ Configuration

-   **Port**: Backend runs on `14000`. Frontend proxy is configured in `client/vite.config.ts`.
-   **Models**: The app automatically fetches available models from your local Ollama instance.
-   **PWA**: Icons/manifest configured in `client/vite.config.ts`.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT License.